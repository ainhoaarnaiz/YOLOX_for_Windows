{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the target folder\n",
    "yolox_dir = \"./YOLOX\"\n",
    "os.chdir(yolox_dir)\n",
    "print(f\"‚ö†Ô∏è Changed working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "def setup_complete_vs_environment():\n",
    "    \"\"\"\n",
    "    Complete Visual Studio environment setup for PyTorch C++ extensions\n",
    "    \"\"\"\n",
    "    vs_path = r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\"\n",
    "    \n",
    "    print(\"Setting up complete Visual Studio environment...\")\n",
    "    \n",
    "    # 1. Find Windows SDK\n",
    "    sdk_bases = [\n",
    "        r\"C:\\Program Files (x86)\\Windows Kits\\10\",\n",
    "        r\"C:\\Program Files\\Windows Kits\\10\"\n",
    "    ]\n",
    "    \n",
    "    include_paths = []\n",
    "    lib_paths = []\n",
    "    \n",
    "    for base in sdk_bases:\n",
    "        if os.path.exists(base):\n",
    "            # Find SDK versions\n",
    "            include_base = os.path.join(base, \"Include\")\n",
    "            lib_base = os.path.join(base, \"Lib\")\n",
    "            \n",
    "            if os.path.exists(include_base):\n",
    "                versions = [d for d in os.listdir(include_base) if d.startswith(\"10.\")]\n",
    "                if versions:\n",
    "                    latest_version = max(versions)\n",
    "                    sdk_include = os.path.join(include_base, latest_version)\n",
    "                    sdk_lib = os.path.join(lib_base, latest_version)\n",
    "                    \n",
    "                    # Add SDK include paths\n",
    "                    include_paths.extend([\n",
    "                        os.path.join(sdk_include, \"ucrt\"),\n",
    "                        os.path.join(sdk_include, \"um\"),\n",
    "                        os.path.join(sdk_include, \"shared\"),\n",
    "                        os.path.join(sdk_include, \"winrt\"),\n",
    "                        os.path.join(sdk_include, \"cppwinrt\"),\n",
    "                    ])\n",
    "                    \n",
    "                    # Add SDK library paths\n",
    "                    lib_paths.extend([\n",
    "                        os.path.join(sdk_lib, \"ucrt\", \"x64\"),\n",
    "                        os.path.join(sdk_lib, \"um\", \"x64\"),\n",
    "                    ])\n",
    "                    \n",
    "                    print(f\"Found Windows SDK {latest_version}\")\n",
    "                    break\n",
    "    \n",
    "    # 2. Find MSVC tools\n",
    "    vc_tools_path = os.path.join(vs_path, \"VC\", \"Tools\", \"MSVC\")\n",
    "    if os.path.exists(vc_tools_path):\n",
    "        versions = os.listdir(vc_tools_path)\n",
    "        if versions:\n",
    "            latest_version = max(versions)\n",
    "            msvc_base = os.path.join(vc_tools_path, latest_version)\n",
    "            \n",
    "            # Add MSVC include paths\n",
    "            include_paths.extend([\n",
    "                os.path.join(msvc_base, \"include\"),\n",
    "                os.path.join(msvc_base, \"atlmfc\", \"include\"),\n",
    "            ])\n",
    "            \n",
    "            # Add MSVC library paths\n",
    "            lib_paths.extend([\n",
    "                os.path.join(msvc_base, \"lib\", \"x64\"),\n",
    "                os.path.join(msvc_base, \"atlmfc\", \"lib\", \"x64\"),\n",
    "            ])\n",
    "            \n",
    "            # Add compiler to PATH\n",
    "            bin_path = os.path.join(msvc_base, \"bin\", \"Hostx64\", \"x64\")\n",
    "            current_path = os.environ.get('PATH', '')\n",
    "            os.environ['PATH'] = f\"{bin_path};{current_path}\"\n",
    "            \n",
    "            print(f\"Found MSVC {latest_version}\")\n",
    "    \n",
    "    # 3. Set environment variables\n",
    "    # Include paths\n",
    "    current_include = os.environ.get('INCLUDE', '')\n",
    "    new_include = ';'.join(include_paths + ([current_include] if current_include else []))\n",
    "    os.environ['INCLUDE'] = new_include\n",
    "    \n",
    "    # Library paths\n",
    "    current_lib = os.environ.get('LIB', '')\n",
    "    new_lib = ';'.join(lib_paths + ([current_lib] if current_lib else []))\n",
    "    os.environ['LIB'] = new_lib\n",
    "    \n",
    "    # Essential VS environment variables\n",
    "    os.environ.update({\n",
    "        'DISTUTILS_USE_SDK': '1',\n",
    "        'MSSdk': '1',\n",
    "        'VS160COMNTOOLS': f\"{vs_path}\\\\Common7\\\\Tools\\\\\",\n",
    "        'VCINSTALLDIR': f\"{vs_path}\\\\VC\\\\\",\n",
    "        'WindowsSDKDir': sdk_bases[0] + \"\\\\\" if os.path.exists(sdk_bases[0]) else \"\",\n",
    "        'PLATFORM': 'x64',\n",
    "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
    "    })\n",
    "    \n",
    "    print(f\"Set up {len(include_paths)} include paths\")\n",
    "    print(f\"Set up {len(lib_paths)} library paths\")\n",
    "    \n",
    "    # 4. Verify key libraries exist\n",
    "    key_libs = ['kernel32.lib', 'msvcprt.lib', 'msvcrt.lib', 'oldnames.lib']\n",
    "    found_libs = {}\n",
    "    \n",
    "    for lib_name in key_libs:\n",
    "        for lib_path in lib_paths:\n",
    "            lib_file = os.path.join(lib_path, lib_name)\n",
    "            if os.path.exists(lib_file):\n",
    "                found_libs[lib_name] = lib_file\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nFound libraries: {list(found_libs.keys())}\")\n",
    "    missing_libs = set(key_libs) - set(found_libs.keys())\n",
    "    if missing_libs:\n",
    "        print(f\"Missing libraries: {list(missing_libs)}\")\n",
    "        \n",
    "        # Try to find them in other locations\n",
    "        print(\"Searching for missing libraries...\")\n",
    "        for lib_name in missing_libs:\n",
    "            for lib_path in lib_paths:\n",
    "                if os.path.exists(lib_path):\n",
    "                    all_libs = [f for f in os.listdir(lib_path) if f.endswith('.lib')]\n",
    "                    similar = [lib for lib in all_libs if lib_name.split('.')[0] in lib]\n",
    "                    if similar:\n",
    "                        print(f\"  In {lib_path}: found similar {similar[:3]}\")\n",
    "    \n",
    "    return len(missing_libs) == 0\n",
    "\n",
    "def clean_torch_cache():\n",
    "    \"\"\"Clean PyTorch extension cache to force recompilation\"\"\"\n",
    "    cache_path = os.path.expanduser(\"~/.cache/torch_extensions\")\n",
    "    if os.path.exists(cache_path):\n",
    "        import shutil\n",
    "        shutil.rmtree(cache_path)\n",
    "        print(\"Cleaned PyTorch extensions cache\")\n",
    "    \n",
    "    # Also clean the specific cache location\n",
    "    local_cache = r\"C:\\Users\\aarnaizl\\AppData\\Local\\torch_extensions\"\n",
    "    if os.path.exists(local_cache):\n",
    "        import shutil\n",
    "        shutil.rmtree(local_cache)\n",
    "        print(\"Cleaned local PyTorch extensions cache\")\n",
    "\n",
    "# Run the setup\n",
    "print(\"=== Setting up Visual Studio Environment ===\")\n",
    "success = setup_complete_vs_environment()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Environment setup complete!\")\n",
    "    print(\"üóëÔ∏è Cleaning PyTorch cache for fresh compilation...\")\n",
    "    clean_torch_cache()\n",
    "    print(\"\\nüöÄ Ready to run training with fast_cocoeval!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some libraries are missing. You may need to:\")\n",
    "    print(\"Check Visual Studio Build Tools is installed with the required modules (see README.md)\")\n",
    "    \n",
    "print(\"\\nYou can now run your training command.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# YOLOX specific imports\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import adjust_status, ModelEMA, postprocess\n",
    "from yolox.data import COCODataset, ValTransform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- CONFIG --------\n",
    "# Use the exact same paths as training\n",
    "data_dir = \"D:/Ainhoa/traffic_signs_data/DFG_detection/dataset_coco_ready_original_yolox\"\n",
    "val_ann = \"coco_val_annotations_remapped.json\"  # Same as training\n",
    "model_weights_path = r'D:\\Ainhoa\\traffic_signs_data\\models\\YOLOX_outputs_dfg_m\\yolox_m\\best_ckpt.pth'\n",
    "output_dir = \"yolox_eval\"\n",
    "class_names_path = \"classes.txt\"\n",
    "\n",
    "# Load experiment configuration (same as training)\n",
    "exp_file = \"yolox_m.py\"  # Your experiment file\n",
    "exp = get_exp(exp_file, None)\n",
    "\n",
    "# -------- LOAD CLASS NAMES --------\n",
    "with open(class_names_path, \"r\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"[INFO] Using experiment: {exp.exp_name}\")\n",
    "print(f\"[INFO] Test size: {exp.test_size}\")\n",
    "print(f\"[INFO] Confidence threshold: {exp.test_conf}\")\n",
    "print(f\"[INFO] NMS threshold: {exp.nmsthre}\")\n",
    "print(f\"[INFO] Number of classes: {exp.num_classes}\")\n",
    "\n",
    "# -------- CONFIGURE AND LOAD MODEL (EXACTLY LIKE TRAINING) --------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = exp.get_model()\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(model_weights_path, map_location=\"cpu\")\n",
    "\n",
    "# Load model weights (regular model first)\n",
    "print(\"Loading regular model weights\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------- CREATE EVALUATOR (EXACTLY LIKE TRAINING) --------\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create evaluator exactly like training\n",
    "evaluator = exp.get_evaluator(\n",
    "    batch_size=1,  # Use batch_size=1 for evaluation\n",
    "    is_distributed=False\n",
    ")\n",
    "\n",
    "print(\"Using regular model for evaluation\")\n",
    "evalmodel = model\n",
    "\n",
    "# -------- RUN EVALUATION (EXACTLY LIKE TRAINING) --------\n",
    "print(\"[INFO] Running evaluation...\")\n",
    "\n",
    "# Adjust model status for evaluation (exactly like training)\n",
    "with adjust_status(evalmodel, training=False):\n",
    "    # Use the exact same evaluation method as training\n",
    "    ap50_95, ap50, summary = exp.eval(\n",
    "        evalmodel, evaluator, is_distributed=False\n",
    "    )\n",
    "\n",
    "# -------- PRINT RESULTS EXACTLY LIKE TRAINING --------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(summary)\n",
    "\n",
    "# -------- GENERATE COCO RESULTS JSON FOR PLOTTING --------\n",
    "print(\"[INFO] Found val2017 subdirectory\")\n",
    "val_dataset = COCODataset(\n",
    "        data_dir=data_dir,\n",
    "        json_file=val_ann,\n",
    "        name=\"val2017\",  # Use val2017 subdirectory\n",
    "        img_size=exp.test_size,\n",
    "        preproc=ValTransform(legacy=False),\n",
    "        cache=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Collect all predictions in COCO format\n",
    "coco_results = []\n",
    "inference_time = 0\n",
    "nms_time = 0\n",
    "n_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cur_iter, (imgs, _, info_imgs, ids) in enumerate(tqdm(val_loader, desc=\"Processing images\")):\n",
    "        imgs = imgs.to(device, non_blocking=True).float()\n",
    "        \n",
    "        # Get image info\n",
    "        img_id = int(ids[0])\n",
    "        img_h = float(info_imgs[0])\n",
    "        img_w = float(info_imgs[1])\n",
    "        \n",
    "        # Inference (exactly like training)\n",
    "        inf_start = time.time()\n",
    "        outputs = evalmodel(imgs)\n",
    "        inf_end = time.time()\n",
    "        inference_time += inf_end - inf_start\n",
    "        \n",
    "        # Postprocessing (exactly like training)\n",
    "        nms_start = time.time()\n",
    "        outputs = postprocess(\n",
    "            outputs, exp.num_classes, exp.test_conf, exp.nmsthre, class_agnostic=True\n",
    "        )\n",
    "        nms_end = time.time()\n",
    "        nms_time += nms_end - nms_start\n",
    "        \n",
    "        # Process results\n",
    "        if outputs[0] is not None:\n",
    "            output = outputs[0].cpu()\n",
    "            bboxes = output[:, 0:4]\n",
    "            \n",
    "            # Scale back to original image (exactly like training)\n",
    "            scale = min(exp.test_size[0] / img_h, exp.test_size[1] / img_w)\n",
    "            bboxes /= scale\n",
    "            \n",
    "            # Extract scores and class IDs\n",
    "            obj_conf = output[:, 4]\n",
    "            cls_conf = output[:, 5]\n",
    "            scores = obj_conf * cls_conf  # Combined confidence\n",
    "            cls_ids = output[:, 6]\n",
    "            \n",
    "            # Convert to COCO format\n",
    "            for i in range(len(bboxes)):\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                \n",
    "                # Only save valid bboxes\n",
    "                if width > 0 and height > 0:\n",
    "                    coco_results.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(cls_ids[i]) + 1,  # COCO categories start from 1\n",
    "                        \"bbox\": [float(x1), float(y1), float(width), float(height)],\n",
    "                        \"score\": float(scores[i])\n",
    "                    })\n",
    "        \n",
    "        n_samples += 1\n",
    "\n",
    "# -------- SAVE RESULTS --------\n",
    "# Save COCO results JSON\n",
    "coco_results_file = os.path.join(output_dir, \"coco_instances_results.json\")\n",
    "with open(coco_results_file, \"w\") as f:\n",
    "    json.dump(coco_results, f)\n",
    "\n",
    "print(f\"\\n[INFO] COCO results saved to: {coco_results_file}\")\n",
    "print(f\"[INFO] Total detections: {len(coco_results)}\")\n",
    "\n",
    "# Save evaluation summary\n",
    "results = {\n",
    "    \"summary\": summary,\n",
    "    \"ap50_95\": ap50_95,\n",
    "    \"ap50\": ap50,\n",
    "    \"total_detections\": len(coco_results),\n",
    "    \"avg_inference_time_ms\": (inference_time / n_samples) * 1000,\n",
    "    \"avg_nms_time_ms\": (nms_time / n_samples) * 1000,\n",
    "}\n",
    "\n",
    "results_file = os.path.join(output_dir, \"training_style_evaluation.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"[INFO] Evaluation summary saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0388727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DetectionEvaluator:\n",
    "    \"\"\"Class to handle object detection evaluation with confusion matrices and PR curves.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.class_names = self._load_class_names()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.coco_gt, self.coco_pred = self._load_data()\n",
    "        self.catid_to_index = self._build_category_mapping()\n",
    "        \n",
    "        os.makedirs(config['output_dir'], exist_ok=True)\n",
    "    \n",
    "    def _load_class_names(self):\n",
    "        \"\"\"Load class names from file.\"\"\"\n",
    "        with open(self.config['class_names_path'], \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load COCO ground truth and prediction data.\"\"\"\n",
    "        with open(self.config['coco_json'], 'r') as f:\n",
    "            coco_gt = json.load(f)\n",
    "        with open(self.config['prediction_json'], 'r') as f:\n",
    "            coco_pred = json.load(f)\n",
    "        return coco_gt, coco_pred\n",
    "    \n",
    "    def _build_category_mapping(self):\n",
    "        \"\"\"Build mapping from category ID to class index.\"\"\"\n",
    "        return {cat['id']: idx for idx, cat in enumerate(self.coco_gt['categories'])}\n",
    "    \n",
    "    def _organize_predictions_by_image(self):\n",
    "        \"\"\"Organize ground truth and predictions by image.\"\"\"\n",
    "        # Ground truth by image\n",
    "        gt_by_image = {}\n",
    "        for ann in self.coco_gt['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            class_idx = self.catid_to_index[ann['category_id']]\n",
    "            gt_by_image.setdefault(img_id, []).append(class_idx)\n",
    "        \n",
    "        # Predictions by image (filtered by confidence)\n",
    "        pred_by_image = {}\n",
    "        min_conf = self.config['min_confidence_thresh']\n",
    "        for pred in self.coco_pred:\n",
    "            if pred[\"score\"] >= min_conf:\n",
    "                img_id = pred[\"image_id\"]\n",
    "                class_id = self.catid_to_index.get(pred[\"category_id\"], -1)\n",
    "                pred_by_image.setdefault(img_id, []).append(class_id)\n",
    "        \n",
    "        return gt_by_image, pred_by_image\n",
    "    \n",
    "    def _prepare_confusion_matrix_data(self, gt_by_image, pred_by_image):\n",
    "        \"\"\"Prepare data for confusion matrix calculation.\"\"\"\n",
    "        all_gts, all_preds = [], []\n",
    "        \n",
    "        for img_id, gts in gt_by_image.items():\n",
    "            preds = pred_by_image.get(img_id, [])\n",
    "            \n",
    "            # Matched predictions\n",
    "            for gt, pred in zip(gts, preds):\n",
    "                all_gts.append(gt)\n",
    "                all_preds.append(pred)\n",
    "            \n",
    "            # Unmatched ground truths (false negatives)\n",
    "            for gt in gts[len(preds):]:\n",
    "                all_gts.append(gt)\n",
    "                all_preds.append(self.num_classes)  # unmatched class\n",
    "            \n",
    "            # Unmatched predictions (false positives)\n",
    "            for pred in preds[len(gts):]:\n",
    "                all_gts.append(self.num_classes)\n",
    "                all_preds.append(pred)\n",
    "        \n",
    "        # Add missing class if needed\n",
    "        if self.num_classes in all_preds or self.num_classes in all_gts:\n",
    "            self.class_names.append(\"_missing_\")\n",
    "        \n",
    "        return all_gts, all_preds\n",
    "    \n",
    "    def plot_confusion_matrices(self, all_gts, all_preds):\n",
    "        \"\"\"Plot confusion matrices (multiclass or binary aggregated).\"\"\"\n",
    "        threshold = self.config['class_display_threshold']\n",
    "        \n",
    "        if self.num_classes <= threshold:\n",
    "            self._plot_multiclass_confusion_matrices(all_gts, all_preds)\n",
    "        else:\n",
    "            self._plot_binary_confusion_matrix(all_gts, all_preds)\n",
    "    \n",
    "    def _plot_multiclass_confusion_matrices(self, all_gts, all_preds):\n",
    "        \"\"\"Plot standard multiclass confusion matrices.\"\"\"\n",
    "        labels = list(range(len(self.class_names)))\n",
    "        configs = [\n",
    "            (confusion_matrix(all_gts, all_preds, labels=labels), \n",
    "             \"Confusion Matrix\", \"confusion_matrix.png\"),\n",
    "            (confusion_matrix(all_gts, all_preds, labels=labels, normalize=\"true\"), \n",
    "             \"Normalized Confusion Matrix\", \"confusion_matrix_normalized.png\")\n",
    "        ]\n",
    "        \n",
    "        for matrix, title, filename in configs:\n",
    "            self._save_confusion_matrix(matrix, title, filename, figsize=(12, 10))\n",
    "    \n",
    "    def _plot_binary_confusion_matrix(self, all_gts, all_preds):\n",
    "        \"\"\"Plot binary confusion matrix (object vs no-object).\"\"\"\n",
    "        y_true_binary = [1 if true < self.num_classes else 0 for true in all_gts]\n",
    "        y_pred_binary = [1 if pred < self.num_classes else 0 for pred in all_preds]\n",
    "        \n",
    "        matrix = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
    "        self._save_confusion_matrix(\n",
    "            matrix, \n",
    "            \"Aggregated Confusion Matrix (Object vs No Object)\", \n",
    "            \"confusion_matrix_aggregated.png\",\n",
    "            display_labels=[\"No Object\", \"Object\"],\n",
    "            figsize=(6, 5)\n",
    "        )\n",
    "    \n",
    "    def _save_confusion_matrix(self, matrix, title, filename, display_labels=None, figsize=(12, 10)):\n",
    "        \"\"\"Save confusion matrix plot.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        labels = display_labels or self.class_names\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=90)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.config['output_dir'], filename))\n",
    "        plt.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_iou(box1, box2):\n",
    "        \"\"\"Calculate IoU between two bounding boxes in [x,y,w,h] format.\"\"\"\n",
    "        x1, y1, w1, h1 = box1\n",
    "        x2, y2, w2, h2 = box2\n",
    "        \n",
    "        # Calculate intersection\n",
    "        x_left = max(x1, x2)\n",
    "        y_top = max(y1, y2)\n",
    "        x_right = min(x1 + w1, x2 + w2)\n",
    "        y_bottom = min(y1 + h1, y2 + h2)\n",
    "        \n",
    "        if x_right < x_left or y_bottom < y_top:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "        union_area = w1 * h1 + w2 * h2 - intersection_area\n",
    "        \n",
    "        return intersection_area / union_area if union_area > 0 else 0.0\n",
    "    \n",
    "    def build_pr_curve_data(self, iou_threshold=0.5):\n",
    "        \"\"\"Build proper PR curve data for object detection with IoU matching.\"\"\"\n",
    "        # Organize ground truth by image and class with bounding boxes\n",
    "        gt_by_image_class = defaultdict(list)\n",
    "        gt_count_per_class = [0] * self.num_classes\n",
    "        \n",
    "        for ann in self.coco_gt['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            class_id = self.catid_to_index.get(ann['category_id'])\n",
    "            if class_id is not None and class_id < self.num_classes:\n",
    "                bbox = ann['bbox']\n",
    "                gt_by_image_class[(img_id, class_id)].append({\n",
    "                    'bbox': bbox,\n",
    "                    'matched': False,\n",
    "                    'ann_id': ann.get('id', len(gt_by_image_class))\n",
    "                })\n",
    "                gt_count_per_class[class_id] += 1\n",
    "        \n",
    "        # Sort all predictions by confidence (descending)\n",
    "        sorted_predictions = sorted(self.coco_pred, key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Initialize arrays for each class\n",
    "        all_scores = [[] for _ in range(self.num_classes)]\n",
    "        all_true = [[] for _ in range(self.num_classes)]\n",
    "        \n",
    "        # Process each prediction in order of decreasing confidence\n",
    "        for pred in sorted_predictions:\n",
    "            class_id = self.catid_to_index.get(pred['category_id'])\n",
    "            if class_id is None or class_id >= self.num_classes:\n",
    "                continue\n",
    "                \n",
    "            img_id = pred['image_id']\n",
    "            pred_bbox = pred['bbox']\n",
    "            confidence = pred['score']\n",
    "            \n",
    "            # Find ground truth objects of the same class in the same image\n",
    "            gt_objects = gt_by_image_class.get((img_id, class_id), [])\n",
    "            \n",
    "            # Find the best matching ground truth (highest IoU)\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt_obj in enumerate(gt_objects):\n",
    "                if not gt_obj['matched']:  # Only consider unmatched ground truths\n",
    "                    iou = self.calculate_iou(pred_bbox, gt_obj['bbox'])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "            \n",
    "            # Record the prediction\n",
    "            all_scores[class_id].append(confidence)\n",
    "            \n",
    "            if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "                # True positive: mark GT as matched\n",
    "                all_true[class_id].append(1)\n",
    "                gt_objects[best_gt_idx]['matched'] = True\n",
    "            else:\n",
    "                # False positive\n",
    "                all_true[class_id].append(0)\n",
    "        \n",
    "        return all_scores, all_true, gt_count_per_class\n",
    "    \n",
    "    def plot_pr_curves(self, all_scores, all_true, gt_count_per_class):\n",
    "        \"\"\"Plot all PR-related curves.\"\"\"\n",
    "        curve_configs = [\n",
    "            (\"PR\", \"PR_curve.png\", \"Recall\", \"Precision\",\n",
    "             lambda p, r, t: r, lambda p, r, t: p),\n",
    "            (\"F1\", \"F1_curve.png\", \"Confidence\", \"F1\",\n",
    "             lambda p, r, t: t, lambda p, r, t: (2 * p[1:] * r[1:] / (p[1:] + r[1:] + 1e-6))),\n",
    "            (\"Prec\", \"P_curve.png\", \"Confidence\", \"Precision\",\n",
    "             lambda p, r, t: t, lambda p, r, t: p[1:]),\n",
    "            (\"Recall\", \"R_curve.png\", \"Confidence\", \"Recall\",\n",
    "             lambda p, r, t: t, lambda p, r, t: r[1:])\n",
    "        ]\n",
    "        \n",
    "        for config in curve_configs:\n",
    "            self._plot_single_curve(config, all_scores, all_true, gt_count_per_class)\n",
    "    \n",
    "    def _plot_single_curve(self, curve_config, all_scores, all_true, gt_count_per_class):\n",
    "        \"\"\"Plot a single PR-related curve.\"\"\"\n",
    "        curve_type, filename, xlabel, ylabel, x_fn, y_fn = curve_config\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        show_legend = self.num_classes <= self.config['class_display_threshold']\n",
    "        \n",
    "        # Plot individual class curves\n",
    "        if show_legend:\n",
    "            class_names_to_plot = (self.class_names[:-1] if \"_missing_\" in self.class_names \n",
    "                                 else self.class_names)\n",
    "            \n",
    "            for i, name in enumerate(class_names_to_plot):\n",
    "                if self._should_skip_class(i, all_scores, gt_count_per_class):\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    self._plot_class_curve(i, name, all_scores, all_true, x_fn, y_fn)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping class {name} due to error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Plot combined curve\n",
    "        self._plot_combined_curve(all_scores, all_true, x_fn, y_fn)\n",
    "        \n",
    "        # Format and save plot\n",
    "        self._format_and_save_plot(curve_type, filename, xlabel, ylabel)\n",
    "    \n",
    "    def _should_skip_class(self, class_idx, all_scores, gt_count_per_class):\n",
    "        \"\"\"Check if a class should be skipped in plotting.\"\"\"\n",
    "        return (len(all_scores[class_idx]) == 0 or \n",
    "                gt_count_per_class[class_idx] == 0 or \n",
    "                len(all_scores[class_idx]) < 3)\n",
    "    \n",
    "    def _plot_class_curve(self, class_idx, class_name, all_scores, all_true, x_fn, y_fn):\n",
    "        \"\"\"Plot curve for a single class.\"\"\"\n",
    "        p, r, t = precision_recall_curve(all_true[class_idx], all_scores[class_idx])\n",
    "        \n",
    "        if len(p) < 2 or len(r) < 2:\n",
    "            return\n",
    "            \n",
    "        x, y = x_fn(p, r, t), y_fn(p, r, t)\n",
    "        min_len = min(len(x), len(y))\n",
    "        plt.plot(x[:min_len], y[:min_len], label=class_name, linewidth=1)\n",
    "    \n",
    "    def _plot_combined_curve(self, all_scores, all_true, x_fn, y_fn):\n",
    "        \"\"\"Plot combined curve for all classes.\"\"\"\n",
    "        try:\n",
    "            all_combined_true = []\n",
    "            all_combined_scores = []\n",
    "            \n",
    "            for i in range(self.num_classes):\n",
    "                all_combined_true.extend(all_true[i])\n",
    "                all_combined_scores.extend(all_scores[i])\n",
    "            \n",
    "            if len(all_combined_true) > 0 and len(set(all_combined_true)) > 1:\n",
    "                p_c, r_c, t_c = precision_recall_curve(all_combined_true, all_combined_scores)\n",
    "                x_c, y_c = x_fn(p_c, r_c, t_c), y_fn(p_c, r_c, t_c)\n",
    "                min_len = min(len(x_c), len(y_c))\n",
    "                plt.plot(x_c[:min_len], y_c[:min_len], \n",
    "                        label=\"all classes\", linewidth=3, color='blue')\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting combined curve: {e}\")\n",
    "    \n",
    "    def _format_and_save_plot(self, curve_type, filename, xlabel, ylabel):\n",
    "        \"\"\"Format and save the plot.\"\"\"\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(filename[:-4])\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        legend_loc = \"lower right\" if curve_type == 'Prec' else \"lower left\"\n",
    "        plt.legend(fontsize='small', loc=legend_loc)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.config['output_dir'], filename))\n",
    "        plt.close()\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Run the complete evaluation pipeline.\"\"\"\n",
    "        print(\"Starting evaluation...\")\n",
    "        \n",
    "        # Confusion matrices\n",
    "        print(\"Generating confusion matrices...\")\n",
    "        gt_by_image, pred_by_image = self._organize_predictions_by_image()\n",
    "        all_gts, all_preds = self._prepare_confusion_matrix_data(gt_by_image, pred_by_image)\n",
    "        self.plot_confusion_matrices(all_gts, all_preds)\n",
    "        \n",
    "        # PR curves\n",
    "        print(\"Generating PR curves...\")\n",
    "        all_scores, all_true, gt_count_per_class = self.build_pr_curve_data()\n",
    "        self.plot_pr_curves(all_scores, all_true, gt_count_per_class)\n",
    "        \n",
    "        print(f\"Evaluation complete! Results saved to {self.config['output_dir']}\")\n",
    "\n",
    "config = {\n",
    "    \"prediction_json\": \"yolox_eval/coco_instances_results.json\",\n",
    "    \"coco_json\": \"D:/Ainhoa/traffic_signs_data/DFG_detection/dataset_coco_ready_original_yolox/annotations/coco_val_annotations_remapped.json\",\n",
    "    \"class_names_path\": \"classes.txt\",\n",
    "    \"output_dir\": \"yolox_eval\",\n",
    "    \"class_display_threshold\": 25,\n",
    "    \"min_confidence_thresh\": 0.5\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = DetectionEvaluator(config)\n",
    "evaluator.run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolox_colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
