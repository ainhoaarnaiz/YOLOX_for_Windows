{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c355eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the target folder\n",
    "yolox_dir = \"./YOLOX\"\n",
    "os.chdir(yolox_dir)\n",
    "print(f\"‚ö†Ô∏è Changed working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ef656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "def setup_complete_vs_environment():\n",
    "    \"\"\"\n",
    "    Complete Visual Studio environment setup for PyTorch C++ extensions\n",
    "    \"\"\"\n",
    "    vs_path = r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\"\n",
    "    \n",
    "    print(\"Setting up complete Visual Studio environment...\")\n",
    "    \n",
    "    # 1. Find Windows SDK\n",
    "    sdk_bases = [\n",
    "        r\"C:\\Program Files (x86)\\Windows Kits\\10\",\n",
    "        r\"C:\\Program Files\\Windows Kits\\10\"\n",
    "    ]\n",
    "    \n",
    "    include_paths = []\n",
    "    lib_paths = []\n",
    "    \n",
    "    for base in sdk_bases:\n",
    "        if os.path.exists(base):\n",
    "            # Find SDK versions\n",
    "            include_base = os.path.join(base, \"Include\")\n",
    "            lib_base = os.path.join(base, \"Lib\")\n",
    "            \n",
    "            if os.path.exists(include_base):\n",
    "                versions = [d for d in os.listdir(include_base) if d.startswith(\"10.\")]\n",
    "                if versions:\n",
    "                    latest_version = max(versions)\n",
    "                    sdk_include = os.path.join(include_base, latest_version)\n",
    "                    sdk_lib = os.path.join(lib_base, latest_version)\n",
    "                    \n",
    "                    # Add SDK include paths\n",
    "                    include_paths.extend([\n",
    "                        os.path.join(sdk_include, \"ucrt\"),\n",
    "                        os.path.join(sdk_include, \"um\"),\n",
    "                        os.path.join(sdk_include, \"shared\"),\n",
    "                        os.path.join(sdk_include, \"winrt\"),\n",
    "                        os.path.join(sdk_include, \"cppwinrt\"),\n",
    "                    ])\n",
    "                    \n",
    "                    # Add SDK library paths\n",
    "                    lib_paths.extend([\n",
    "                        os.path.join(sdk_lib, \"ucrt\", \"x64\"),\n",
    "                        os.path.join(sdk_lib, \"um\", \"x64\"),\n",
    "                    ])\n",
    "                    \n",
    "                    print(f\"Found Windows SDK {latest_version}\")\n",
    "                    break\n",
    "    \n",
    "    # 2. Find MSVC tools\n",
    "    vc_tools_path = os.path.join(vs_path, \"VC\", \"Tools\", \"MSVC\")\n",
    "    if os.path.exists(vc_tools_path):\n",
    "        versions = os.listdir(vc_tools_path)\n",
    "        if versions:\n",
    "            latest_version = max(versions)\n",
    "            msvc_base = os.path.join(vc_tools_path, latest_version)\n",
    "            \n",
    "            # Add MSVC include paths\n",
    "            include_paths.extend([\n",
    "                os.path.join(msvc_base, \"include\"),\n",
    "                os.path.join(msvc_base, \"atlmfc\", \"include\"),\n",
    "            ])\n",
    "            \n",
    "            # Add MSVC library paths\n",
    "            lib_paths.extend([\n",
    "                os.path.join(msvc_base, \"lib\", \"x64\"),\n",
    "                os.path.join(msvc_base, \"atlmfc\", \"lib\", \"x64\"),\n",
    "            ])\n",
    "            \n",
    "            # Add compiler to PATH\n",
    "            bin_path = os.path.join(msvc_base, \"bin\", \"Hostx64\", \"x64\")\n",
    "            current_path = os.environ.get('PATH', '')\n",
    "            os.environ['PATH'] = f\"{bin_path};{current_path}\"\n",
    "            \n",
    "            print(f\"Found MSVC {latest_version}\")\n",
    "    \n",
    "    # 3. Set environment variables\n",
    "    # Include paths\n",
    "    current_include = os.environ.get('INCLUDE', '')\n",
    "    new_include = ';'.join(include_paths + ([current_include] if current_include else []))\n",
    "    os.environ['INCLUDE'] = new_include\n",
    "    \n",
    "    # Library paths\n",
    "    current_lib = os.environ.get('LIB', '')\n",
    "    new_lib = ';'.join(lib_paths + ([current_lib] if current_lib else []))\n",
    "    os.environ['LIB'] = new_lib\n",
    "    \n",
    "    # Essential VS environment variables\n",
    "    os.environ.update({\n",
    "        'DISTUTILS_USE_SDK': '1',\n",
    "        'MSSdk': '1',\n",
    "        'VS160COMNTOOLS': f\"{vs_path}\\\\Common7\\\\Tools\\\\\",\n",
    "        'VCINSTALLDIR': f\"{vs_path}\\\\VC\\\\\",\n",
    "        'WindowsSDKDir': sdk_bases[0] + \"\\\\\" if os.path.exists(sdk_bases[0]) else \"\",\n",
    "        'PLATFORM': 'x64',\n",
    "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
    "    })\n",
    "    \n",
    "    print(f\"Set up {len(include_paths)} include paths\")\n",
    "    print(f\"Set up {len(lib_paths)} library paths\")\n",
    "    \n",
    "    # 4. Verify key libraries exist\n",
    "    key_libs = ['kernel32.lib', 'msvcprt.lib', 'msvcrt.lib', 'oldnames.lib']\n",
    "    found_libs = {}\n",
    "    \n",
    "    for lib_name in key_libs:\n",
    "        for lib_path in lib_paths:\n",
    "            lib_file = os.path.join(lib_path, lib_name)\n",
    "            if os.path.exists(lib_file):\n",
    "                found_libs[lib_name] = lib_file\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nFound libraries: {list(found_libs.keys())}\")\n",
    "    missing_libs = set(key_libs) - set(found_libs.keys())\n",
    "    if missing_libs:\n",
    "        print(f\"Missing libraries: {list(missing_libs)}\")\n",
    "        \n",
    "        # Try to find them in other locations\n",
    "        print(\"Searching for missing libraries...\")\n",
    "        for lib_name in missing_libs:\n",
    "            for lib_path in lib_paths:\n",
    "                if os.path.exists(lib_path):\n",
    "                    all_libs = [f for f in os.listdir(lib_path) if f.endswith('.lib')]\n",
    "                    similar = [lib for lib in all_libs if lib_name.split('.')[0] in lib]\n",
    "                    if similar:\n",
    "                        print(f\"  In {lib_path}: found similar {similar[:3]}\")\n",
    "    \n",
    "    return len(missing_libs) == 0\n",
    "\n",
    "def clean_torch_cache():\n",
    "    \"\"\"Clean PyTorch extension cache to force recompilation\"\"\"\n",
    "    cache_path = os.path.expanduser(\"~/.cache/torch_extensions\")\n",
    "    if os.path.exists(cache_path):\n",
    "        import shutil\n",
    "        shutil.rmtree(cache_path)\n",
    "        print(\"Cleaned PyTorch extensions cache\")\n",
    "    \n",
    "    # Also clean the specific cache location\n",
    "    local_cache = r\"C:\\Users\\aarnaizl\\AppData\\Local\\torch_extensions\"\n",
    "    if os.path.exists(local_cache):\n",
    "        import shutil\n",
    "        shutil.rmtree(local_cache)\n",
    "        print(\"Cleaned local PyTorch extensions cache\")\n",
    "\n",
    "# Run the setup\n",
    "print(\"=== Setting up Visual Studio Environment ===\")\n",
    "success = setup_complete_vs_environment()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Environment setup complete!\")\n",
    "    print(\"üóëÔ∏è Cleaning PyTorch cache for fresh compilation...\")\n",
    "    clean_torch_cache()\n",
    "    print(\"\\nüöÄ Ready to run training with fast_cocoeval!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some libraries are missing. You may need to:\")\n",
    "    print(\"Check Visual Studio Build Tools is installed with the required modules (see README.md)\")\n",
    "    \n",
    "print(\"\\nYou can now run your training command.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5259905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import postprocess, vis\n",
    "\n",
    "# === CONFIG ===\n",
    "model_path = r'D:\\Ainhoa\\traffic_signs_data\\models\\YOLOX_outputs_dfg_m\\yolox_m\\best_ckpt.pth'\n",
    "video_path = \"videos/dfg_traffic_sign_demo.mp4\"\n",
    "output_dir = \"videos\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_filename = os.path.basename(video_path)\n",
    "output_video_path = os.path.join(output_dir, f\"detected_{video_filename}\")\n",
    "\n",
    "# YOLOX model configuration\n",
    "exp_file = None  # Path to experiment file, or None for default\n",
    "model_name = \"yolox-m\"  # Options: yolox-nano, yolox-tiny, yolox-s, yolox-m, yolox-l, yolox-x\n",
    "input_size = (1024, 1024)  # Model input size\n",
    "conf_thresh = 0.7\n",
    "nms_thresh = 0.65\n",
    "\n",
    "# Load class names\n",
    "class_names_txt = \"classes.txt\"\n",
    "with open(class_names_txt, 'r') as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# === SETUP YOLOX MODEL ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get experiment configuration\n",
    "if exp_file is None:\n",
    "    exp = get_exp(None, model_name)\n",
    "else:\n",
    "    exp = get_exp(exp_file, None)\n",
    "\n",
    "# Update experiment config\n",
    "exp.num_classes = len(class_names)\n",
    "exp.test_conf = conf_thresh\n",
    "exp.nmsthre = nms_thresh\n",
    "exp.test_size = input_size\n",
    "\n",
    "# Build model\n",
    "model = exp.get_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load weights\n",
    "ckpt = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "print(f\"[INFO] Loaded YOLOX model from {model_path}\")\n",
    "\n",
    "def preprocess(img, input_size):\n",
    "    \"\"\"Preprocess image for YOLOX\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "    else:\n",
    "        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "    padded_img = padded_img.transpose((2, 0, 1))\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    \n",
    "    return padded_img, r\n",
    "\n",
    "def draw_detections(img, outputs, ratio, class_names):\n",
    "    \"\"\"Draw bounding boxes and labels on image\"\"\"\n",
    "    if outputs is None:\n",
    "        return img\n",
    "    \n",
    "    bboxes = outputs[:, 0:4]\n",
    "    cls = outputs[:, 6]\n",
    "    scores = outputs[:, 4] * outputs[:, 5]\n",
    "    \n",
    "    # Scale bboxes back to original image size\n",
    "    bboxes /= ratio\n",
    "    \n",
    "    for i, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cls_id = int(cls[i])\n",
    "        score = scores[i]\n",
    "        \n",
    "        if cls_id < len(class_names):\n",
    "            class_name = class_names[cls_id]\n",
    "        else:\n",
    "            class_name = f\"class_{cls_id}\"\n",
    "        \n",
    "        # Draw green bounding box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label_text = f\"{class_name} {score:.2f}\"\n",
    "        cv2.putText(img, label_text, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, \n",
    "                   lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# === READ VIDEO ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# === SAVE VIDEO SETUP ===\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"[INFO] Processing video: {video_path}\")\n",
    "print(f\"[INFO] Saving output to: {output_video_path}\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "frame_count = 0\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # === PREPROCESS ===\n",
    "        img, ratio = preprocess(frame, input_size)\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # === INFERENCE ===\n",
    "        outputs = model(img)\n",
    "        outputs = postprocess(outputs, exp.num_classes, conf_thresh, nms_thresh)[0]\n",
    "\n",
    "        # === DRAW DETECTIONS ===\n",
    "        if outputs is not None:\n",
    "            frame = draw_detections(frame, outputs.cpu().numpy(), ratio, class_names)\n",
    "\n",
    "        # === WRITE FRAME ===\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"[INFO] Processed {frame_count} frames\")\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"[‚úÖ] Done. Processed {frame_count} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from yolox.exp import get_exp\n",
    "# from yolox.utils import postprocess, vis\n",
    "\n",
    "# # === CONFIG ===\n",
    "# model_path = r'D:\\Ainhoa\\traffic_signs_data\\models\\YOLOX_outputs_dfg_m_single_class\\yolox_m\\best_ckpt.pth'\n",
    "# video_path = \"videos/dfg_traffic_sign_demo.mp4\"\n",
    "# output_dir = \"videos\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# video_filename = os.path.basename(video_path)\n",
    "# output_video_path = os.path.join(output_dir, f\"detected_{video_filename}\")\n",
    "# detections_json_path = os.path.join(output_dir, f\"detections_{video_filename.split('.')[0]}.json\")\n",
    "\n",
    "# # YOLOX model configuration\n",
    "# exp_file = None  # Path to experiment file, or None for default\n",
    "# model_name = \"yolox-m\"  # Options: yolox-nano, yolox-tiny, yolox-s, yolox-m, yolox-l, yolox-x\n",
    "# input_size = (1024, 1024)  # Model input size\n",
    "# conf_thresh = 0.7\n",
    "# nms_thresh = 0.65\n",
    "\n",
    "# # Load class names\n",
    "# class_names_txt = \"class_names.txt\"\n",
    "# with open(class_names_txt, 'r') as f:\n",
    "#     class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# # === SETUP YOLOX MODEL ===\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Get experiment configuration\n",
    "# if exp_file is None:\n",
    "#     exp = get_exp(None, model_name)\n",
    "# else:\n",
    "#     exp = get_exp(exp_file, None)\n",
    "\n",
    "# # Update experiment config\n",
    "# exp.num_classes = len(class_names)\n",
    "# exp.test_conf = conf_thresh\n",
    "# exp.nmsthre = nms_thresh\n",
    "# exp.test_size = input_size\n",
    "\n",
    "# # Build model\n",
    "# model = exp.get_model()\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Load weights\n",
    "# ckpt = torch.load(model_path, map_location=device)\n",
    "# model.load_state_dict(ckpt[\"model\"])\n",
    "# print(f\"[INFO] Loaded YOLOX model from {model_path}\")\n",
    "\n",
    "# def preprocess(img, input_size):\n",
    "#     \"\"\"Preprocess image for YOLOX\"\"\"\n",
    "#     if len(img.shape) == 3:\n",
    "#         padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "#     else:\n",
    "#         padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "\n",
    "#     r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "#     resized_img = cv2.resize(\n",
    "#         img,\n",
    "#         (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "#         interpolation=cv2.INTER_LINEAR,\n",
    "#     ).astype(np.uint8)\n",
    "    \n",
    "#     padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "#     padded_img = padded_img.transpose((2, 0, 1))\n",
    "#     padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    \n",
    "#     return padded_img, r\n",
    "\n",
    "# def extract_detections(outputs, ratio, class_names, frame_number):\n",
    "#     \"\"\"Extract detection information for JSON export\"\"\"\n",
    "#     detections = []\n",
    "    \n",
    "#     if outputs is None:\n",
    "#         return detections\n",
    "    \n",
    "#     bboxes = outputs[:, 0:4]\n",
    "#     cls = outputs[:, 6]\n",
    "#     scores = outputs[:, 4] * outputs[:, 5]\n",
    "    \n",
    "#     # Scale bboxes back to original image size\n",
    "#     bboxes /= ratio\n",
    "    \n",
    "#     for i, box in enumerate(bboxes):\n",
    "#         x1, y1, x2, y2 = box.astype(int)\n",
    "#         cls_id = int(cls[i])\n",
    "#         score = float(scores[i])\n",
    "        \n",
    "#         if cls_id < len(class_names):\n",
    "#             class_name = class_names[cls_id]\n",
    "#         else:\n",
    "#             class_name = f\"class_{cls_id}\"\n",
    "        \n",
    "#         detection = {\n",
    "#             \"frame\": frame_number,\n",
    "#             \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
    "#             \"class_id\": cls_id,\n",
    "#             \"class_name\": class_name,\n",
    "#             \"confidence\": score,\n",
    "#             \"detection_id\": f\"frame_{frame_number}_det_{i}\"\n",
    "#         }\n",
    "#         detections.append(detection)\n",
    "    \n",
    "#     return detections\n",
    "\n",
    "# def draw_detections(img, outputs, ratio, class_names):\n",
    "#     \"\"\"Draw bounding boxes and labels on image\"\"\"\n",
    "#     if outputs is None:\n",
    "#         return img\n",
    "    \n",
    "#     bboxes = outputs[:, 0:4]\n",
    "#     cls = outputs[:, 6]\n",
    "#     scores = outputs[:, 4] * outputs[:, 5]\n",
    "    \n",
    "#     # Scale bboxes back to original image size\n",
    "#     bboxes /= ratio\n",
    "    \n",
    "#     for i, box in enumerate(bboxes):\n",
    "#         x1, y1, x2, y2 = box.astype(int)\n",
    "#         cls_id = int(cls[i])\n",
    "#         score = scores[i]\n",
    "        \n",
    "#         if cls_id < len(class_names):\n",
    "#             class_name = class_names[cls_id]\n",
    "#         else:\n",
    "#             class_name = f\"class_{cls_id}\"\n",
    "        \n",
    "#         # Draw green bounding box\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "#         # Draw label\n",
    "#         label_text = f\"{class_name} {score:.2f}\"\n",
    "#         cv2.putText(img, label_text, (x1, y1 - 10), \n",
    "#                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, \n",
    "#                    lineType=cv2.LINE_AA)\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# def save_detection_crops(frame, outputs, ratio, frame_number, crops_dir):\n",
    "#     \"\"\"Save cropped images of detections for classification\"\"\"\n",
    "#     if outputs is None:\n",
    "#         return []\n",
    "    \n",
    "#     os.makedirs(crops_dir, exist_ok=True)\n",
    "#     crop_paths = []\n",
    "    \n",
    "#     bboxes = outputs[:, 0:4]\n",
    "#     cls = outputs[:, 6]\n",
    "#     scores = outputs[:, 4] * outputs[:, 5]\n",
    "    \n",
    "#     # Scale bboxes back to original image size\n",
    "#     bboxes /= ratio\n",
    "    \n",
    "#     for i, box in enumerate(bboxes):\n",
    "#         x1, y1, x2, y2 = box.astype(int)\n",
    "        \n",
    "#         # Ensure coordinates are within frame bounds\n",
    "#         h, w = frame.shape[:2]\n",
    "#         x1, y1 = max(0, x1), max(0, y1)\n",
    "#         x2, y2 = min(w, x2), min(h, y2)\n",
    "        \n",
    "#         # Crop the detection\n",
    "#         crop = frame[y1:y2, x1:x2]\n",
    "        \n",
    "#         # Save crop\n",
    "#         crop_filename = f\"frame_{frame_number}_det_{i}.jpg\"\n",
    "#         crop_path = os.path.join(crops_dir, crop_filename)\n",
    "#         cv2.imwrite(crop_path, crop)\n",
    "#         crop_paths.append(crop_path)\n",
    "    \n",
    "#     return crop_paths\n",
    "\n",
    "# # === READ VIDEO ===\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# # === SAVE VIDEO SETUP ===\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# # Setup for saving detection crops\n",
    "# crops_dir = os.path.join(output_dir, \"detection_crops\")\n",
    "\n",
    "# print(f\"[INFO] Processing video: {video_path}\")\n",
    "# print(f\"[INFO] Saving output to: {output_video_path}\")\n",
    "# print(f\"[INFO] Saving detections to: {detections_json_path}\")\n",
    "# print(f\"[INFO] Saving detection crops to: {crops_dir}\")\n",
    "# print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# frame_count = 0\n",
    "# all_detections = []\n",
    "\n",
    "# # Video metadata\n",
    "# video_metadata = {\n",
    "#     \"video_path\": video_path,\n",
    "#     \"video_filename\": video_filename,\n",
    "#     \"width\": width,\n",
    "#     \"height\": height,\n",
    "#     \"fps\": fps,\n",
    "#     \"model_config\": {\n",
    "#         \"model_name\": model_name,\n",
    "#         \"input_size\": input_size,\n",
    "#         \"conf_thresh\": conf_thresh,\n",
    "#         \"nms_thresh\": nms_thresh\n",
    "#     },\n",
    "#     \"class_names\": class_names\n",
    "# }\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # === PREPROCESS ===\n",
    "#         img, ratio = preprocess(frame, input_size)\n",
    "#         img = torch.from_numpy(img).unsqueeze(0).float().to(device)\n",
    "\n",
    "#         # === INFERENCE ===\n",
    "#         outputs = model(img)\n",
    "#         outputs = postprocess(outputs, exp.num_classes, conf_thresh, nms_thresh)[0]\n",
    "\n",
    "#         # === EXTRACT DETECTIONS FOR JSON ===\n",
    "#         frame_detections = extract_detections(\n",
    "#             outputs.cpu().numpy() if outputs is not None else None, \n",
    "#             ratio, class_names, frame_count\n",
    "#         )\n",
    "#         all_detections.extend(frame_detections)\n",
    "\n",
    "#         # === SAVE DETECTION CROPS ===\n",
    "#         if outputs is not None:\n",
    "#             crop_paths = save_detection_crops(frame, outputs.cpu().numpy(), ratio, frame_count, crops_dir)\n",
    "#             # Add crop paths to detections\n",
    "#             for det, crop_path in zip(frame_detections, crop_paths):\n",
    "#                 det[\"crop_path\"] = crop_path\n",
    "\n",
    "#         # === DRAW DETECTIONS ===\n",
    "#         if outputs is not None:\n",
    "#             frame = draw_detections(frame, outputs.cpu().numpy(), ratio, class_names)\n",
    "\n",
    "#         # === WRITE FRAME ===\n",
    "#         out.write(frame)\n",
    "#         frame_count += 1\n",
    "        \n",
    "#         if frame_count % 100 == 0:\n",
    "#             print(f\"[INFO] Processed {frame_count} frames\")\n",
    "\n",
    "# cap.release()\n",
    "# out.release()\n",
    "\n",
    "# # === SAVE DETECTIONS TO JSON ===\n",
    "# detection_data = {\n",
    "#     \"video_metadata\": video_metadata,\n",
    "#     \"total_frames\": frame_count,\n",
    "#     \"total_detections\": len(all_detections),\n",
    "#     \"detections\": all_detections\n",
    "# }\n",
    "\n",
    "# with open(detections_json_path, 'w') as f:\n",
    "#     json.dump(detection_data, f, indent=2)\n",
    "\n",
    "# print(f\"[‚úÖ] Done. Processed {frame_count} frames.\")\n",
    "# print(f\"[‚úÖ] Saved {len(all_detections)} detections to {detections_json_path}\")\n",
    "# print(f\"[‚úÖ] Saved detection crops to {crops_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolox_colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
